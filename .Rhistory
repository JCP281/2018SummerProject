pearson.groups <- cutree(dist.avg.timbre.spearman, k = 10)
table(spearman.groups)
dist.avg.timbre.spearman <- hclust(as.dist(1-cor(avg.timbre, method="spearman")), method="complete")
spearman.groups <- cutree(dist.avg.timbre.spearman, k = 10)
table(spearman.groups)
avg.timbre.spearman <- mutate(avg.timbre, spearman.groups)
plot(avg.timbre.pearson)
plot(pearson.groups)
dist.avg.timbre.spearman <- hclust(as.dist(1-cor(avg.timbre, method="spearman")), method="complete")
spearman.groups <- cutree(dist.avg.timbre.spearman, k = 10)
table(spearman.groups)
dist.avg.timbre.pearson <- hclust(as.dist(1-cor(avg.timbre, method="pearson")), method="complete")
pearson.groups <- cutree(dist.avg.timbre.pearson, k = 10)
table(pearson.groups)
# Add cutree results to existing matrix
avg.timbre.pearson <- mutate(avg.timbre, pearson.groups)
dist.avg.timbre.pearson <- hclust(as.dist(1-cor(t(avg.timbre), method="pearson")), method="complete")
pearson.groups <- cutree(dist.avg.timbre.pearson, k = 10)
table(pearson.groups)
# Add cutree results to existing matrix
avg.timbre.pearson <- mutate(avg.timbre, pearson.groups)
dist.avg.timbre.spearman <- hclust(as.dist(1-cor(t(avg.timbre), method="spearman")), method="complete")
spearman.groups <- cutree(dist.avg.timbre.spearman, k = 10)
table(spearman.groups)
# Add cutree results to existing matrix
avg.timbre.spearman <- mutate(avg.timbre, spearman.groups)
plot(avg.timbre.pearson)
plot(avg.timbre.spearman)
knitr::opts_chunk$set(echo = TRUE)
hw2.df <- read.csv('/Users/MOS/Documents/UTSA/Data Driven Decsn Mking & Design (DA-6213)/Homework/HW2/pr.csv')
head(hw2.df)
cor(hw2.df)
pairs(hw2.df)
library(corrplot)
corrplot(cor(hw2.df), order = 'hclust', method = 'number')
model <- lm(y ~ .,data = hw2.df)
hw2.pc <- prcomp(hw2.df[,-7], center = TRUE, scale. = TRUE)
summary(hw2.pc)
model <- lm(y ~ x1 + x2 + x3, hw2.df)
summary(model)
yearPredict <- read.csv('YearPredictionMSD.txt')
sum(is.na(yearPredict))  # Look for missing values
dim(yearPredict)         # Get size and shape of data
library(Hmisc)  # Needed for hclust() function
library(dplyr)  # Needed for mutate() function
avg.timbre <- yearPredict[1:5000,2:13]  # r crashes with entire group, so take subset
dist.avg.timbre.pearson <- hclust(as.dist(1-cor(t(avg.timbre), method="pearson")), method="complete")
pearson.groups <- cutree(dist.avg.timbre.pearson, k = 10)
table(pearson.groups)
# Add cutree results to existing matrix
avg.timbre.pearson <- mutate(avg.timbre, pearson.groups)
dist.avg.timbre.spearman <- hclust(as.dist(1-cor(t(avg.timbre), method="spearman")), method="complete")
spearman.groups <- cutree(dist.avg.timbre.spearman, k = 10)
table(spearman.groups)
# Add cutree results to existing matrix
avg.timbre.spearman <- mutate(avg.timbre, spearman.groups)
knitr::opts_chunk$set(echo = TRUE)
hw2.df <- read.csv('/Users/MOS/Documents/UTSA/Data Driven Decsn Mking & Design (DA-6213)/Homework/HW2/pr.csv')
head(hw2.df)
cor(hw2.df)
pairs(hw2.df)
library(corrplot)
corrplot(cor(hw2.df), order = 'hclust', method = 'number')
model <- lm(y ~ .,data = hw2.df)
hw2.pc <- prcomp(hw2.df[,-7], center = TRUE, scale. = TRUE)
summary(hw2.pc)
model <- lm(y ~ x1 + x2 + x3, hw2.df)
summary(model)
yearPredict <- read.csv('YearPredictionMSD.txt')
sum(is.na(yearPredict))  # Look for missing values
dim(yearPredict)         # Get size and shape of data
library(Hmisc)  # Needed for hclust() function
library(dplyr)  # Needed for mutate() function
avg.timbre <- yearPredict[1:5000,2:13]  # r crashes with entire group, so take subset
dist.avg.timbre.pearson <- hclust(as.dist(1-cor(t(avg.timbre), method="pearson")), method="complete")
pearson.groups <- cutree(dist.avg.timbre.pearson, k = 10)
table(pearson.groups)
dist.avg.timbre.spearman <- hclust(as.dist(1-cor(t(avg.timbre), method="spearman")), method="complete")
spearman.groups <- cutree(dist.avg.timbre.spearman, k = 10)
table(spearman.groups)
library(Hmisc)  # Needed for hclust() function
library(dplyr)  # Needed for mutate() function
avg.timbre <- yearPredict[1:5000,2:13]  # r crashes with entire group, so take subset
dist.avg.timbre.pearson <- hclust(as.dist(1-cor(t(avg.timbre), method="pearson")), method="complete")
pearson.groups <- cutree(dist.avg.timbre.pearson, k = 10)
table(pearson.groups)
dist.avg.timbre.spearman <- hclust(as.dist(1-cor(avg.timbre, method="spearman")), method="complete")
spearman.groups <- cutree(dist.avg.timbre.spearman, k = 10)
table(spearman.groups)
avg.timbre <- yearPredict[1:10000,2:13]  # r crashes with entire group, so take subset
# Disimilarity matrix (1-r^2) using Pearson
c<-1-cor(avg.timbre)^2
c
dist.avg.timbre.pearson <- hclust(as.dist(1-cor(t(avg.timbre)^2, method="pearson")), method="complete")
pearson.groups <- cutree(dist.avg.timbre.pearson, k = 10)
table(pearson.groups)
pearson.groups
dist.avg.timbre.spearman <- hclust(as.dist(1-cor(t(avg.timbre)^2, method="spearman")), method="complete")
spearman.groups <- cutree(dist.avg.timbre.spearman, k = 10)
table(spearman.groups)
spearman.groups
plot(spearman.groups)
plot(pearson.groups)
library(Hmisc)  # Needed for hclust() function
library(dplyr)  # Needed for mutate() function
library(factoextra)
avg.timbre <- yearPredict[1:10000,2:13]  # r crashes with entire group, so take subset
distances = dist(avg.timbre, method = "euclidean")
fit_hc<-hclust(distances, method = "pearson")
distances = dist(avg.timbre, method = "pearson")
distances = get_dist(avg.timbre, method = "pearson")
fit_hc<-hclust(distances, method = "ward.D")
plot(fit_hc)
rect.hclust(fit_hc, k = 10, border = "blue")
distances = get_dist(avg.timbre, method = "spearman")
fit_hc<-hclust(distances, method = "ward.D")
plot(fit_hc)
rect.hclust(fit_hc, k = 10, border = "blue")
knitr::opts_chunk$set(echo = TRUE)
# Subset of the data
set.seed(123)
ss <- sample(1:463715, 10000) # Take 10,000 random rows
avg.timbre <- yearPredict[ss, 2:13] # Subset the 10 rows
avg.timbre.scaled <- scale(avg.timbre)      # Since we are using different scales
dist.pears <- as.dist(1-cor(t(avg.timbre.scaled), method='pearson'))
hc.pears <- hclust(dist.pears, method='average')
pears.groups <- cutree(hc.pears, k = 10)
table(pears.groups)
par(cex = 0.2)
plot(hc.pears, main = 'Pearson Correlation')
rect.hclust(hc.pears, k = 10, border = 'blue')
# Compute cophentic distance
res.coph <- cophenetic(hc.pears)
# Correlation between cophenetic distance and the original distance
cor(dist.pears, res.coph)
par(cex = 0)
plot(hc.pears, main = 'Pearson Correlation', labels = FALSE)
dim(avg.timbre)
res.coph <- cophenetic(hc.pears)
# Correlation between cophenetic distance and the original distance
cor(dist.pears, res.coph)
table(pears.groups)
knitr::opts_chunk$set(echo = TRUE)
library(cluster)
# DIvisive ANAlysis Clustering
hc.d <- diana(x = avg.timbre,      # data matrix
stand = TRUE,        # standardize the data
metric = "euclidean" # metric for distance matrix
)
d.groups <- cutree(as.hclust(hc.d), k = 10)
table(d.groups)
?boxcox
??boxcox
library(MASS)
?boxcox
Accidents0515 <- read.csv('Accidents0515.csv')
Casualties0515 <- read.csv('Casualties0515.csv')
Vehicles0515 <- read.csv('Vehicles0515.csv')
typeof(Accidents0515)
as.data.frame(Accidents0515)
typeof(Accidents0515)
Accidents0515 <- data.frame(Accidents0515)
typeof(Accidents0515)
Accidents0515 <- map(Accidents0515, data_frame)
library(purrr)
Accidents0515 <- map(Accidents0515, data_frame)
Accidents0515 <- as.data.frame(Accidents0515)
typeof(Accidents0515)
str(Accidents0515)
Accidents0515 <- read.csv('Accidents0515.csv')
str(Accidents0515)
sqldf("select * from Accidents0515 limit 10")
library(sqldf)
sqldf("select * from Accidents0515 limit 10")
sqldf("select * from Accidents0515 where Date = '05/09/2013'")
sqldf("select distinct Latitude from Accidents0515 where Date = '05/09/2013")
sqldf("select distinct Latitude from Accidents0515 where Date = '05/09/2013'")
sqldf("select count(Latitude) from Accidents0515 where Date = '05/09/2013'")
acc.uk <- merge(Accidents0515, Casualties0515, by="Accident_Index")
sqldf("select count(Latitude) from acc.uk where Date = '05/09/2013'")
rm(acc.uk)
glimpse(Accidents0515)
library(dplyr)
glimpse(Accidents0515)
summary(Accidents0515)
library(sqldf)
sqldf("SELECT *
FROM Accidents0515 a
INNER JOIN Casualties0515 c ON c.AccidentIndex = a.AccidentIndex
INNER JOIN Vehicles0515 v ON v.AccidentIndex = c.AccidentIndex")
library(sqldf)
sqldf("SELECT *
FROM Accidents0515 a
INNER JOIN Casualties0515 c ON c.Accident_Index = a.Accident_Index
INNER JOIN Vehicles0515 v ON v.Accident_Index = c.Accident_Index")
library(sqldf)
sqldf("SELECT COUNT(*)
FROM Accidents0515 a
INNER JOIN Casualties0515 c ON c.Accident_Index = a.Accident_Index
INNER JOIN Vehicles0515 v ON v.Accident_Index = c.Accident_Index")
library(sqldf)
acc.uk <- sqldf("SELECT *
FROM Accidents0515 a
INNER JOIN Casualties0515 c ON c.Accident_Index = a.Accident_Index
INNER JOIN Vehicles0515 v ON v.Accident_Index = c.Accident_Index")
sqldf("select count(*) from acc.uk")
rm(acc.uk)
head(Accidents0515)
knitr::opts_chunk$set(echo = TRUE)
library(sqldf)
sqldf("SELECT COUNT(Junction_Control)
FROM Accidents0515 a
INNER JOIN Casualties0515 c ON c.Accident_Index = a.Accident_Index
INNER JOIN Vehicles0515 v ON v.Accident_Index = c.Accident_Index
WHERE Junction_Control = -1")
glimpse(Accidents0515)
library(dplyr)
glimpse(Accidents0515)
glimpse(Vehicles0515)
sqldf("select Accident_Index from Accidents0515 where speed_limit = 0")
sqldf("select count(Accident_Index) from Accidents0515 where speed_limit = 0")
sqldf("select count(Accident_Index) from Vehicles0515 where age_of_vehicle = -1")
sum(Vehicless0515$Age_Of_Vehicle == -1)
sum(Vehicles0515$Age_Of_Vehicle == -1)
sum(Vehicles0515$Age_Of_Vehicle == -1)
length(which(Vehicles0515$Age_Of_Vehicle == -1))
which(Vehicles0515$Age_Of_Vehicle == -1)
Vehicles0515$Age_of_Vehicle
length(Vehicles0515$Age_of_Vehicle==-1)
length(Vehicles0515$Age_of_Vehicle!=-1)
which(Vehicles0515$Age_of_Vehicle==-1)
length(which(Vehicles0515$Age_of_Vehicle==-1))
length(which(Vehicles0515$Age_of_Vehicle!=-1))
Accidents0515 <- read.csv('Accidents0515.csv')
setwd('/Users/MOS/Documents/GitHub/2018SummerProject/')
Accidents0515 <- read.csv('Accidents0515.csv')
setwd('/Users/MOS/Documents/GitHub/2018SummerProject/')
Accidents0515 <- read.csv('Accidents0515.csv')
Casualties0515 <- read.csv('Casualties0515.csv')
Vehicles0515 <- read.csv('Vehicles0515.csv')
glimpse(Accidents0515)
glimpse(Casualties0515)
glimpse(Vehicles0515)
min(Accidents0515$Number_of_Vehicles)
min(Accidents0515$Day_of_Week)
min(Accidents0515$Date)
min(Accidents0515$Time)
min(Accidents0515$Road_Surface_Conditions)
length(which(Accidents0515$Road_Surface_Conditions))
length(which(Accidents0515$Road_Surface_Conditions==-1))
length(which(Accidents0515[,27]==-1))
length(which(Accidents0515$Speed_limit==0))
length(which(Accidents0515$X1st_Road_Number==-1))
apply(Accidents0515[,19:32], 2, function(x){length(which(x)==-1)})
apply(Accidents0515[,19:21], 2, function(x){length(which(x)==-1)})
apply(Accidents0515[,19], 2, function(x){length(which(x)==-1)})
length(which(Accidents0515[,27]==-1))
apply(Accidents0515[,-1:18], 2, function(x){length(which(x)==-1)})
head(Accidents0515[,19:32])
which(Accidents0515[,19:32]) == -1
which(Accidents0515[,19:32] == -1)
length(which(Accidents0515[,19:32] == -1))
length(which(Accidents0515[,19] == -1))
length(which(Accidents0515[,20] == -1))
length(which(Accidents0515[,21] == -1))
length(which(Accidents0515[,22] == -1))
length(which(Accidents0515[,23] == -1))
length(which(Accidents0515[,24] == -1))
length(which(Accidents0515[,25] == -1))
length(which(Accidents0515[,26] == -1))
length(which(Accidents0515[,27] == -1))
length(which(Accidents0515[,28] == -1))
length(which(Accidents0515[,29] == -1))
length(which(Accidents0515[,30] == -1))
length(which(Accidents0515[,31] == -1))
length(which(Accidents0515[,32] == -1))
memory.limit(size = 1000000000)
?memory.limit
?memory
?`Memory-limits`
?mice()
?VIM::mice
knitr::opts_chunk$set(echo = TRUE)
library(mice)
install.packages("mice")
library(mice)
?mice
VIM::kNN(Accidents0515)
Accidents0515 <- read.csv('Accidents0515.csv')
knitr::opts_chunk$set(echo = TRUE)
library(caret)
set.seed(123)
data(tecator)
pr = prcomp(absorp)
vars = pr$sdev^2
total_var = sum(vars)
if(save_plots){
postscript("../../WriteUp/Graphics/Chapter6/chap_6_prob_1_scree_plot.eps", onefile=FALSE, horizontal=FALSE) }
knitr::opts_chunk$set(echo = TRUE)
library(Hmisc)
library(AppliedPredictiveModeling)
set.seed(123)
# Generate some class labels (of nonuniform classes):
#
n = 1000
classes = sample(c(1,2,3), n, replace=TRUE, prob=c(0.7,0.2,0.1))
# Show that our class data gives the correct proporations:
#
print(table(classes)/n)
# Demonstrate the use of createDataPartition:
#
split = createDataPartition(classes, p=0.8)
# Verify that we have a stratified sample that "matches" our prior distribution:
#
T = table(classes[ split$Resample1 ])
print(T/sum(T))
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(Hmisc)
library(AppliedPredictiveModeling)
library(pracma)
install.packages("pracma")
data
length(data)
length(Accidents0515$Accident_Index)
summary(Accidents0515)
prcntJnctnCtrlCounts0
knitr::opts_chunk$set(echo = TRUE)
memory.limit(size = 1000000000)
Accidents0515$X2nd_Road_Class[Accidents0515$X2nd_Road_Class == -1] <- 0
numNonMissJnctnCtrlCounts <- length(
Accidents0515$Junction_Control[Accidents0515$Junction_Control != -1])
prcntJnctnCtrlCounts0 <- length(
Accidents0515$Junction_Control[Accidents0515$Junction_Control == 0]) / numNonMissJnctnCtrlCounts
prcntJnctnCtrlCounts1 <- length(
Accidents0515$Junction_Control[Accidents0515$Junction_Control == 1]) / numNonMissJnctnCtrlCounts
prcntJnctnCtrlCounts2 <- length(
Accidents0515$Junction_Control[Accidents0515$Junction_Control == 2]) / numNonMissJnctnCtrlCounts
prcntJnctnCtrlCounts3 <- length(
Accidents0515$Junction_Control[Accidents0515$Junction_Control == 3]) / numNonMissJnctnCtrlCounts
prcntJnctnCtrlCounts4 <- length(
Accidents0515$Junction_Control[Accidents0515$Junction_Control == 4]) / numNonMissJnctnCtrlCounts
numMissJnctnCtrlCounts <- length(
Accidents0515$Junction_Control[Accidents0515$Junction_Control == -1])
numReplaceWith0 <- round(prcntJnctnCtrlCounts0 * numMissJnctnCtrlCounts,0)
numReplaceWith1 <- round(prcntJnctnCtrlCounts1 * numMissJnctnCtrlCounts,0)
numReplaceWith2 <- round(prcntJnctnCtrlCounts2 * numMissJnctnCtrlCounts,0)
numReplaceWith3 <- round(prcntJnctnCtrlCounts3 * numMissJnctnCtrlCounts,0)
numReplaceWith4 <- round(prcntJnctnCtrlCounts4 * numMissJnctnCtrlCounts,0)
ndxMissingJnctnCntrl <- which(Accidents0515$Junction_Control == -1)
data <- c(
rep(0,numReplaceWith0), rep(1,numReplaceWith1), rep(2,numReplaceWith2),
rep(3,numReplaceWith3), rep(4,numReplaceWith4))
seed(123)
seed(123)
library(imputeR)
library(dplyr)
seed(123)
set.seed(123)
data <- data[sample(1:length(data))]
Accidents0515$Junction_Control[ndxMissingJnctnCntrl[1]] <- data[1]
length(Accidents0515$Junction_Control[Accidents0515$Junction_Control == -1])
Accidents0515$Junction_Control[ndxMissingJnctnCntrl[1]] <- data[1]
Accidents0515$Junction_Control[ndxMissingJnctnCntrl[2]] <- data[2]
length(Accidents0515$Junction_Control[Accidents0515$Junction_Control == -1])
for(i in 1:length(ndxMissingJnctnCntrl)) {
Accidents0515$Junction_Control[ndxMissingJnctnCntrl[i]] <- data[i]
}
library(sqldf)
sqldf("SELECT *
FROM Accidents0515
WHERE Junction_Control = -1")
summary(Accidents0515)
library(imputeR)
install.packages("imputeR")
library(dplyr)
Accidents0515 <- read.csv('Accidents0515.csv')
Accidents0515 = Accidents0515[ , -c(16,22)]  #Removed the street numbers from the data set. I dont feel that this is relevant to anything.
wreckHour = as.character(Accidents0515$Time)
Time_of_Accident = sapply(strsplit(wreckHour,':'), function(x){
x = as.numeric(x)
x[1]+x[2]/60
}
)
Accidents0515 = cbind(Accidents0515,Time_of_Accident)
Accidents0515 = Accidents0515[ , -12]  #Removed old time column that is now converted to numeric as Time_of_Accident
summary(Accidents0515)
guessImpute = Accidents0515[ , c(17,20,21,23,24,25,26,28,30)]
summary(guessImpute)
###################
#Do the modeling for Junction_Control and X2nd_Road_Class BEFORE the guess imputation
# Impute X2nd_Road_Class
# X2nd_Road_Class is undefined wherever Junction_Control is 0, so replace -1 with 0
Accidents0515$X2nd_Road_Class[Accidents0515$X2nd_Road_Class == -1] <- 0
summary(Accidents0515)
Junction_Control_Imputed <- Accidents0515$Junction_Control
cbind(Accidents0515, Junction_Control_Imputed)
numNonMissJnctnCtrlCounts <- length(
Accidents0515$Junction_Control[Accidents0515$Junction_Control != -1])
#for(i in 0:4){
#  paste(prcntJnctnCtrlCounts,as.character(i)) <- round(length(
#    Accidents0515$Junction_Control[Accidents0515$Junction_Control == i])
#    / numNonMissJnctnCtrlCounts * 100,0)
#}
# Determine the proportion of each of the non-nissing values in the column
# We will replace the missing values using these percentages
# This is not very elegant, but its late....
prcntJnctnCtrlCounts0 <- length(
Accidents0515$Junction_Control[Accidents0515$Junction_Control == 0]) / numNonMissJnctnCtrlCounts
prcntJnctnCtrlCounts1 <- length(
Accidents0515$Junction_Control[Accidents0515$Junction_Control == 1]) / numNonMissJnctnCtrlCounts
prcntJnctnCtrlCounts2 <- length(
Accidents0515$Junction_Control[Accidents0515$Junction_Control == 2]) / numNonMissJnctnCtrlCounts
prcntJnctnCtrlCounts3 <- length(
Accidents0515$Junction_Control[Accidents0515$Junction_Control == 3]) / numNonMissJnctnCtrlCounts
prcntJnctnCtrlCounts4 <- length(
Accidents0515$Junction_Control[Accidents0515$Junction_Control == 4]) / numNonMissJnctnCtrlCounts
# Determine the number of values we need to impute
numMissJnctnCtrlCounts <- length(
Accidents0515$Junction_Control[Accidents0515$Junction_Control == -1])
numReplaceWith0 <- round(prcntJnctnCtrlCounts0 * numMissJnctnCtrlCounts,0)
numReplaceWith1 <- round(prcntJnctnCtrlCounts1 * numMissJnctnCtrlCounts,0)
numReplaceWith2 <- round(prcntJnctnCtrlCounts2 * numMissJnctnCtrlCounts,0)
numReplaceWith3 <- round(prcntJnctnCtrlCounts3 * numMissJnctnCtrlCounts,0)
numReplaceWith4 <- round(prcntJnctnCtrlCounts4 * numMissJnctnCtrlCounts,0)
# Get the indexes of the missing values and randomly shuffle them
# This will allow us to randomly assign imputation values
ndxMissingJnctnCntrl <- which(Accidents0515$Junction_Control == -1)
# Create a vector with the replacement values
data <- c(
rep(0,numReplaceWith0), rep(1,numReplaceWith1), rep(2,numReplaceWith2),
rep(3,numReplaceWith3), rep(4,numReplaceWith4))
set.seed(123)
data <- data[sample(1:length(data))]
# Randomly assign the new values to the imputed column
for(i in 1:length(ndxMissingJnctnCntrl)) {
Accidents0515$Junction_Control_Imputed[ndxMissingJnctnCntrl[i]] <- data[i]
}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(Hmisc)
library(pracma)
library(AppliedPredictiveModeling)
library(AppliedPredictiveModeling)
data(ChemicalManufacturingProcess)
?write.csv
wrtie.csv(Accidents0515, 'Accidents0515_MOS.csv')
library(utils)
wrtie.csv(Accidents0515, 'Accidents0515_MOS.csv')
wrtie.table(Accidents0515, 'Accidents0515_MOS.csv')
wrtie.csv(Accidents0515, "Accidents0515_MOS.csv")
summary(Accidents0515)
setwd("~/Documents/GitHub/2018SummerProject")
wrtie.csv(Accidents0515, "Accidents0515_MOS.csv", row.names = FALSE)
wrtie.table(Accidents0515, "Accidents0515_MOS.csv", row.names = FALSE)
str(Accidents0515)
typeof(Accidents0515)
getwd()
library(utils)
utils::wrtie.csv(Accidents0515, "Accidents0515_MOS.csv", row.names = FALSE)
library(csv)
help("write.csv")
MASS::wrtie.csv(Accidents0515, "Accidents0515_MOS.csv", row.names = FALSE)
ls("package:utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
install.packages("utils")
knitr::opts_chunk$set(echo = TRUE)
memory.limit(size = 1000000000)
wrtie.csv(as.matrix(Accidents0515), "Accidents0515_MOS.csv", row.names = FALSE)
?write.csv
wrtie.csv(Accidents0515, file = "Accidents0515_MOS.csv", row.names = FALSE)
write.csv(Accidents0515, file = "Accidents0515_MOS.csv", row.names = FALSE)
names(Accidents0515)
ls("package:caret")
library(caret)
ls("package:caret")
summary(Accidents0515)
Accidents0515 <- Accidents0515[, -1]
summary(Accidents0515)
table(Accidents0515$Date)
summary(Accidents0515)
Accidents0515$newDate <- as.Date(Accidents0515$Date, '%d/%m/%Y')
summary(Accidents0515)
Accidents0515$year<-year(Accidents0515$newDate)
Accidents0515$year<-year(Accidents0515$newDate)
?year
library(lubridate)
Accidents0515$year<-year(Accidents0515$newDate)
#extract month
Accidents0515$month<-as.factor(month(Accidents0515$newDate))
#extract week of  year
Accidents0515$week<-as.factor(week(Accidents0515$newDate))
#extract day of year
Accidents0515$day<-yday(Accidents0515$newDate)
# time slot
Accidents0515$time_slot <-as.numeric(substr(Accidents0515$time,0,2))
Accidents0515$time_slot <-as.numeric(substr(Accidents0515$time,0,2))
Accidents0515$day_of_week<-factor(Accidents0515$day_of_week)
summary(Accidents0515)
table(Accidents0515$Number_of_Casualties)
?glm.nb
library(MASS)
glm.nb(Number_of_Casualties ~ year, Accidents0515)
